[{"authors":["admin"],"categories":null,"content":"{first name}souza{last name} at gmail dot com\nI'm a Researcher at 3778 Healthcare working with Machine Learning for Healthcare. Previously, I was Head of Data Science at Linx Impulse and a Researcher at the CERTI Foundation where I worked with Signal Processing and Embedded Systems. I'm interested in learning more about the intersection between Statistical Learning and Deep Learning as well as how Machine Learning can be seen as an Information Theory problem. I'm also interested in their applications to Health and Privacy. Originally, I am from Florianópolis, Brazil but I've lived in New Jersey, Orlando, Toronto and São Paulo as well as other smaller cities in the south of Brazil. I enjoy reading, playing american football and KSP  ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"{first name}souza{last name} at gmail dot com\nI'm a Researcher at 3778 Healthcare working with Machine Learning for Healthcare. Previously, I was Head of Data Science at Linx Impulse and a Researcher at the CERTI Foundation where I worked with Signal Processing and Embedded Systems. I'm interested in learning more about the intersection between Statistical Learning and Deep Learning as well as how Machine Learning can be seen as an Information Theory problem.","tags":null,"title":"Daniel Severo","type":"authors"},{"authors":null,"categories":[],"content":"","date":1569214244,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569214244,"objectID":"5413b958f2da9adf83440b315bfba04e","permalink":"/work/ziggurat/","publishdate":"2019-09-23T01:50:44-03:00","relpermalink":"/work/ziggurat/","section":"work","summary":"Mathematical proof of functionality, of a highly efficient pseudo-random number generator: The Ziggurat Method","tags":[],"title":"A Report on the Ziggurat Method","type":"work"},{"authors":null,"categories":null,"content":" Creating custom reports and machine learning models with pandas can be cumbersome with limited hardware resources (memory and CPU). Financial constraints can make spawning cloud instances to side-step this issue a problem, while adding the complexity of libraries such as Apache Spark isn\u0026rsquo;t worth the trouble and staggers data exploration. How can we keep the simplicity and power of pandas, while extending it to be out-of-core and parallel?\nEnter Dask: a flexible parallel computing library for analytic computing. With it we will create a linear regression model to predict read time in Medium posts using a Kaggle dataset, while comparing the equivalent implementation with pandas.\n1. Kaggle data We will be using the official kaggle api to automate our data fetching process.\n Log on to kaggle and enter the How good is your Medium article? competition. Configure the official kaggle api following these steps.  For this tutorial we will need only 1 file.\nkaggle competitions download -c how-good-is-your-medium-article -f train.json.gz  The reason for decompressing will become clear later.\ngunzip -k ~/.kaggle/competitions/how-good-is-your-medium-article/train.json.gz  This sample file will help us speed up the analysis.\nhead -n5 ~/.kaggle/competitions/how-good-is-your-medium-article/train.json \u0026gt; \\ ~/.kaggle/competitions/how-good-is-your-medium-article/train-sample.json  2. Exploration. Despite the extension being json our data is stored as jsonl. This means that each line of train.json is a valid json file.\nhead -n1 ~/.kaggle/competitions/how-good-is-your-medium-article/train.json | jq 'del(.content)'  { \u0026quot;_id\u0026quot;: \u0026quot;https://medium.com/policy/medium-terms-of-service-9db0094a1e0f\u0026quot;, \u0026quot;_timestamp\u0026quot;: 1520035195.282891, \u0026quot;_spider\u0026quot;: \u0026quot;medium\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://medium.com/policy/medium-terms-of-service-9db0094a1e0f\u0026quot;, \u0026quot;domain\u0026quot;: \u0026quot;medium.com\u0026quot;, \u0026quot;published\u0026quot;: { \u0026quot;$date\u0026quot;: \u0026quot;2012-08-13T22:54:53.510Z\u0026quot; }, \u0026quot;title\u0026quot;: \u0026quot;Medium Terms of Service – Medium Policy – Medium\u0026quot;, \u0026quot;author\u0026quot;: { \u0026quot;name\u0026quot;: null, \u0026quot;url\u0026quot;: \u0026quot;https://medium.com/@Medium\u0026quot;, \u0026quot;twitter\u0026quot;: \u0026quot;@Medium\u0026quot; }, \u0026quot;image_url\u0026quot;: null, \u0026quot;tags\u0026quot;: [], \u0026quot;link_tags\u0026quot;: { \u0026quot;canonical\u0026quot;: \u0026quot;https://medium.com/policy/medium-terms-of-service-9db0094a1e0f\u0026quot;, \u0026quot;publisher\u0026quot;: \u0026quot;https://plus.google.com/103654360130207659246\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;https://medium.com/@Medium\u0026quot;, \u0026quot;search\u0026quot;: \u0026quot;/osd.xml\u0026quot;, \u0026quot;alternate\u0026quot;: \u0026quot;android-app://com.medium.reader/https/medium.com/p/9db0094a1e0f\u0026quot;, \u0026quot;stylesheet\u0026quot;: \u0026quot;https://cdn-static-1.medium.com/_/fp/css/main-branding-base.Ch8g7KPCoGXbtKfJaVXo_w.css\u0026quot;, \u0026quot;icon\u0026quot;: \u0026quot;https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico\u0026quot;, \u0026quot;apple-touch-icon\u0026quot;: \u0026quot;https://cdn-images-1.medium.com/fit/c/120/120/1*6_fgYnisCa9V21mymySIvA.png\u0026quot;, \u0026quot;mask-icon\u0026quot;: \u0026quot;https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg\u0026quot; }, \u0026quot;meta_tags\u0026quot;: { \u0026quot;viewport\u0026quot;: \u0026quot;width=device-width, initial-scale=1\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Medium Terms of Service – Medium Policy – Medium\u0026quot;, \u0026quot;referrer\u0026quot;: \u0026quot;unsafe-url\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…\u0026quot;, \u0026quot;theme-color\u0026quot;: \u0026quot;#000000\u0026quot;, \u0026quot;og:title\u0026quot;: \u0026quot;Medium Terms of Service – Medium Policy – Medium\u0026quot;, \u0026quot;og:url\u0026quot;: \u0026quot;https://medium.com/policy/medium-terms-of-service-9db0094a1e0f\u0026quot;, \u0026quot;fb:app_id\u0026quot;: \u0026quot;542599432471018\u0026quot;, \u0026quot;og:description\u0026quot;: \u0026quot;These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…\u0026quot;, \u0026quot;twitter:description\u0026quot;: \u0026quot;These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;Medium\u0026quot;, \u0026quot;og:type\u0026quot;: \u0026quot;article\u0026quot;, \u0026quot;twitter:card\u0026quot;: \u0026quot;summary\u0026quot;, \u0026quot;article:publisher\u0026quot;: \u0026quot;https://www.facebook.com/medium\u0026quot;, \u0026quot;article:author\u0026quot;: \u0026quot;https://medium.com/@Medium\u0026quot;, \u0026quot;robots\u0026quot;: \u0026quot;index, follow\u0026quot;, \u0026quot;article:published_time\u0026quot;: \u0026quot;2012-08-13T22:54:53.510Z\u0026quot;, \u0026quot;twitter:creator\u0026quot;: \u0026quot;@Medium\u0026quot;, \u0026quot;twitter:site\u0026quot;: \u0026quot;@Medium\u0026quot;, \u0026quot;og:site_name\u0026quot;: \u0026quot;Medium\u0026quot;, \u0026quot;twitter:label1\u0026quot;: \u0026quot;Reading time\u0026quot;, \u0026quot;twitter:data1\u0026quot;: \u0026quot;5 min read\u0026quot;, \u0026quot;twitter:app:name:iphone\u0026quot;: \u0026quot;Medium\u0026quot;, \u0026quot;twitter:app:id:iphone\u0026quot;: \u0026quot;828256236\u0026quot;, \u0026quot;twitter:app:url:iphone\u0026quot;: \u0026quot;medium://p/9db0094a1e0f\u0026quot;, \u0026quot;al:ios:app_name\u0026quot;: \u0026quot;Medium\u0026quot;, \u0026quot;al:ios:app_store_id\u0026quot;: \u0026quot;828256236\u0026quot;, \u0026quot;al:android:package\u0026quot;: \u0026quot;com.medium.reader\u0026quot;, \u0026quot;al:android:app_name\u0026quot;: \u0026quot;Medium\u0026quot;, \u0026quot;al:ios:url\u0026quot;: \u0026quot;medium://p/9db0094a1e0f\u0026quot;, \u0026quot;al:android:url\u0026quot;: \u0026quot;medium://p/9db0094a1e0f\u0026quot;, \u0026quot;al:web:url\u0026quot;: \u0026quot;https://medium.com/policy/medium-terms-of-service-9db0094a1e0f\u0026quot; } }  I\u0026rsquo;ve ommited the content field due to it\u0026rsquo;s huge verbosity. Our problem requires that we use the fields published.$date and meta_tags.twitter:data.\nhead -n10 train.json | jq '[.published[\u0026quot;$date\u0026quot;], .meta_tags[\u0026quot;twitter:data1\u0026quot;]] | @csv' -r  \u0026quot;2012-08-13T22:54:53.510Z\u0026quot;,\u0026quot;5 min read\u0026quot; \u0026quot;2015-08-03T07:44:50.331Z\u0026quot;,\u0026quot;7 min read\u0026quot; \u0026quot;2017-02-05T13:08:17.410Z\u0026quot;,\u0026quot;2 min read\u0026quot; \u0026quot;2017-05-06T08:16:30.776Z\u0026quot;,\u0026quot;3 min read\u0026quot; \u0026quot;2017-06-04T14:46:25.772Z\u0026quot;,\u0026quot;4 min read\u0026quot; \u0026quot;2017-04-02T16:21:15.171Z\u0026quot;,\u0026quot;7 min read\u0026quot; \u0026quot;2016-08-15T04:16:02.103Z\u0026quot;,\u0026quot;12 min read\u0026quot; \u0026quot;2015-01-14T21:31:07.568Z\u0026quot;,\u0026quot;5 min read\u0026quot; \u0026quot;2014-02-11T04:11:54.771Z\u0026quot;,\u0026quot;4 min read\u0026quot; \u0026quot;2015-10-25T02:58:05.551Z\u0026quot;,\u0026quot;8 min read\u0026quot;  3. Building the time-series: The good, the bad and the ugly. %matplotlib inline  import json import pandas as pd import numpy as np import os import dask.bag as db from toolz.curried import get from typing import Dict HOME = os.environ['HOME'] KAGGLE_DATASET_HOME = '.kaggle/competitions/how-good-is-your-medium-article/' train_file = f'{HOME}/{KAGGLE_DATASET_HOME}/train.json' train_sample_file = f'{HOME}/{KAGGLE_DATASET_HOME}/train-sample.json' MEGABYTES = 1024**2  The Ugly read_json loads each json as a record, parsing each object beforehand.\n( pd .read_json(train_sample_file, lines=True) [['published', 'meta_tags']] )   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    published meta_tags     0 {'$date': '2012-08-13T22:54:53.510Z'} {'viewport': 'width=device-width, initial-scal...   1 {'$date': '2015-08-03T07:44:50.331Z'} {'viewport': 'width=device-width, initial-scal...   2 {'$date': '2017-02-05T13:08:17.410Z'} {'viewport': 'width=device-width, initial-scal...   3 {'$date': '2017-05-06T08:16:30.776Z'} {'viewport': 'width=device-width, initial-scal...   4 {'$date': '2017-06-04T14:46:25.772Z'} {'viewport': 'width=device-width, initial-scal...     Both columns have object values. Our fields of interest can be extracted and assigned to a new column using the assign function.\n( _ .assign( published_timestamp = lambda df: df['published'].apply(dict.get, args=('$date',)), read_time = lambda df: df['meta_tags'].apply(dict.get, args=('twitter:data1',)), ) )   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    published meta_tags published_timestamp read_time     0 {'$date': '2012-08-13T22:54:53.510Z'} {'viewport': 'width=device-width, initial-scal... 2012-08-13T22:54:53.510Z 5 min read   1 {'$date': '2015-08-03T07:44:50.331Z'} {'viewport': 'width=device-width, initial-scal... 2015-08-03T07:44:50.331Z 7 min read   2 {'$date': '2017-02-05T13:08:17.410Z'} {'viewport': 'width=device-width, initial-scal... 2017-02-05T13:08:17.410Z 2 min read   3 {'$date': '2017-05-06T08:16:30.776Z'} {'viewport': 'width=device-width, initial-scal... 2017-05-06T08:16:30.776Z 3 min read   4 {'$date': '2017-06-04T14:46:25.772Z'} {'viewport': 'width=device-width, initial-scal... 2017-06-04T14:46:25.772Z 4 min read     Extracting the time value in read_time can be done with pd.Series.str processing methods. When called, the equivalent function is applied to each value, hence .str.split(' ').str[0] is equivalent to '5 min read'.split(' ')[0].\nastype casts our columns to the necessary dtypes.\n( _ .assign(read_time = lambda df: df['read_time'].str.split(' ').str[0]) .astype({ 'read_time': int, 'published_timestamp': 'datetime64[ns]' }) .set_index('published_timestamp') ['read_time'] .to_frame() )   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    read_time   published_timestamp      2012-08-13 22:54:53.510 5   2015-08-03 07:44:50.331 7   2017-02-05 13:08:17.410 2   2017-05-06 08:16:30.776 3   2017-06-04 14:46:25.772 4     The Bad The issue with The Ugly solution is that read_json loads the entire dataset into memory before slicing the necessary columns (published and meta_tags). Pre-processing our data with pure python consumes less RAM.\ndef make_datum(x): return { 'published_timestamp': x['published']['$date'], 'read_time': x['meta_tags']['twitter:data1'] } with open(train_sample_file, 'r') as f: bad_df = pd.DataFrame([make_datum(json.loads(x)) for x in f])  bad_df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    published_timestamp read_time     0 2012-08-13T22:54:53.510Z 5 min read   1 2015-08-03T07:44:50.331Z 7 min read   2 2017-02-05T13:08:17.410Z 2 min read   3 2017-05-06T08:16:30.776Z 3 min read   4 2017-06-04T14:46:25.772Z 4 min read     ( _ .assign(read_time = lambda x: x['read_time'].str.split(' ').str[0]) .astype({ 'published_timestamp': 'datetime64[ns]', 'read_time': int }) .set_index('published_timestamp') ['read_time'] .to_frame() )   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    read_time   published_timestamp      2012-08-13 22:54:53.510 5   2015-08-03 07:44:50.331 7   2017-02-05 13:08:17.410 2   2017-05-06 08:16:30.776 3   2017-06-04 14:46:25.772 4     The Good Dask allows us to build lazy computational graphs. For example, db.read_text will return a reference to each line of our jsonl file. After, .map applies json.loads to each line and .to_dataframe casts the data to a dask DataFrame preserving only the columns we explicitly tell it (in this case published and meta_tags). The rest of the code proceeds analogously with the previous implementations. The only difference is that dask won\u0026rsquo;t actually process anything until we call the .compute method, returning a pandas DataFrame. In other words, a dask DataFrame is a lazy version of a pandas DataFrame. The same is true for series.\nNotice how we pass the blocksize parameter as 100 MB. Since our file has 2 GB, dask creates 20 independent partitions. Most methods that are called (like .map and .assign) run in parallel, potentially speeding up computation significantly. Memory is also spared, since we only load the fields we need.\ndag = ( db .read_text(train_file, blocksize=100*MEGABYTES) .map(json.loads) .to_dataframe({ 'published': object, 'meta_tags': object }) .assign( published_timestamp=lambda df: df['published'].apply(get('$date')), read_time=lambda df: df['meta_tags'].apply(get('twitter:data1')).str.split(' ').str[0], ) .astype({ 'published_timestamp': 'datetime64[ns]', 'read_time': int }) [['published_timestamp', 'read_time']] ) dag  Dask DataFrame Structure:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    published_timestamp read_time   npartitions=20        datetime64[ns] int64    ... ...   ... ... ...    ... ...    ... ...     Dask Name: getitem, 120 tasks Other methods like .head(N) also force the dataframe to be computed. Since this call needs only the first N rows, dask will partially solve the graph such that only those are processed.\n( _ .head() .set_index('published_timestamp') )   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    read_time   published_timestamp      2012-08-13 22:54:53.510 5   2015-08-03 07:44:50.331 7   2017-02-05 13:08:17.410 2   2017-05-06 08:16:30.776 3   2017-06-04 14:46:25.772 4     4. Prediction Here we will implement simple linear regression for a single variable with intercept: $y = \\alpha + \\beta x$, the closed form solution is:\n$$\\beta = \\frac{cov(x,y)}{var(x)}$$ $$\\alpha = \\bar{y} - \\beta \\bar{x}$$\nwhere $\\bar{y}$ and $\\bar{x}$ are the average values of the vectors $y$ and $x$, respectively.\ndef linear_regression(y: np.array, x: np.array, prefix='') -\u0026gt; Dict[str, float]: M = np.cov(x, y) beta = M[0,1]/M[0,0] alpha = y.mean() - beta*x.mean() return { prefix + 'alpha': alpha, prefix + 'beta': beta }  df = ( dag .compute() .set_index('published_timestamp') ['2015':] ['read_time'] .groupby(lambda i: pd.to_datetime(i.strftime('%Y/%m'))) .agg(['mean', 'sum']) ) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    mean sum     2015-01-01 8.380952 4928   2015-02-01 7.887564 4630   2015-03-01 7.907840 5749   2015-04-01 7.667149 5298   2015-05-01 8.307506 6862     df.plot( style=['o--', 'og--'], figsize=(12,6), subplots=True, title='Medium read time', fontsize=12 );  df_pred = ( df .assign(**linear_regression(df['mean'], df.index.asi8, prefix='mean_')) .assign(**linear_regression(df['sum'], df.index.asi8, prefix='sum_')) .assign(mean_pred = lambda z: z['mean_alpha'] + z['mean_beta']*z.index.asi8) .assign(sum_pred = lambda z: z['sum_alpha'] + z['sum_beta']*z.index.asi8) ) df_pred[['mean_alpha', 'mean_beta', 'sum_alpha', 'sum_beta', 'mean_pred', 'sum_pred']].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    mean_alpha mean_beta sum_alpha sum_beta mean_pred sum_pred     2015-01-01 27.130799 -1.351069e-17 -407905.660648 2.892460e-13 7.944669 2844.030846   2015-02-01 27.130799 -1.351069e-17 -407905.660648 2.892460e-13 7.908482 3618.747348   2015-03-01 27.130799 -1.351069e-17 -407905.660648 2.892460e-13 7.875797 4318.491286   2015-04-01 27.130799 -1.351069e-17 -407905.660648 2.892460e-13 7.839610 5093.207789   2015-05-01 27.130799 -1.351069e-17 -407905.660648 2.892460e-13 7.804590 5842.933436     ( df_pred [['mean', 'mean_pred']] .plot(figsize=(12,5), style=['o--', '--']) );  ( df_pred [['sum', 'sum_pred']] .plot(figsize=(12,5), style=['o--', '--']) );  ","date":1521417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521417600,"objectID":"fb5d20128f00fa11b7ffbbdceddb0359","permalink":"/work/dask/","publishdate":"2018-03-19T00:00:00Z","relpermalink":"/work/dask/","section":"work","summary":"How can we keep the simplicity and power of pandas, while extending it to be out-of-core and parallel?","tags":null,"title":"Ad hoc Big Data Analysis with Dask","type":"work"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c4ed11351608279928032a6eb74e6f37","permalink":"/opensource/dask/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/opensource/dask/","section":"opensource","summary":"Parallel computing with task scheduling","tags":null,"title":"Dask","type":"opensource"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2177a3e7f615bb63287da6aadfdc05b","permalink":"/opensource/dask-ml/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/opensource/dask-ml/","section":"opensource","summary":"Scalable Machine Learn with Dask","tags":null,"title":"Dask-ML","type":"opensource"}]